{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation of Binary addition\n",
    "\n",
    "## Contents\n",
    "1. Generate sample data\n",
    "2. Build Tensorflow RNN model\n",
    "3. Train model\n",
    "4. Calculator wrapper\n",
    "\n",
    "Can be configured in 32, 16, and 8-bit modes however since the binary addition operation generalizes perfectly for each time step the bitwidth doesn't make much difference. For the same reason very few training examples are required to train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "from IPython import display\n",
    "from tensorflow.contrib.rnn import BasicRNNCell\n",
    "\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# Max unsigned integer values\n",
    "max_uint32 = 2 ** 32 - 1\n",
    "max_uint16 = 2 ** 16 - 1\n",
    "max_uint8 = 2 ** 8 - 1\n",
    "\n",
    "# RNN implementation\n",
    "# RNN can be implemented using tensorflow api for RNN or by manually unrolling sequence.\n",
    "use_tf_rnn_api = False\n",
    "\n",
    "# Adder datatype\n",
    "# supports\n",
    "# uint32, uint16 and uint8\n",
    "dtype = \"uint32\"\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 3e-3\n",
    "hidden_neurons = 16\n",
    "\n",
    "# Training data\n",
    "samples = 256\n",
    "train_test_batches_split = 0.5\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampleData(object):\n",
    "\n",
    "    def __init__(self, samples, dtype, batch_size, train_test_batches_split):\n",
    "        valid_dtypes = {\"uint8\", \"uint16\", \"uint32\"}\n",
    "        if not dtype in valid_dtypes: raise ValueError(\"input dtype not in valid dtypes\")\n",
    "        if (samples % batch_size != 0): raise ValueError(\"samples must be multiple of batch_size\")\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        \n",
    "        if dtype == \"uint8\":\n",
    "            self.bitwidth = 8\n",
    "        if dtype == \"uint16\":\n",
    "            self.bitwidth = 16\n",
    "        if dtype == \"uint32\":\n",
    "            self.bitwidth = 32\n",
    "        \n",
    "        self.x0_uint, self.x1_uint = self.gen_x(samples, dtype)\n",
    "        self.y_uint = self.calc_y(self.x0_uint, self.x1_uint, dtype)\n",
    "        \n",
    "        self.x0_bits = arr2inbits(self.x0_uint)\n",
    "        self.x1_bits = arr2inbits(self.x1_uint)\n",
    "        self.y_bits = arr2inbits(self.y_uint)\n",
    "        self.x0_samples_bits = np.reshape(self.x0_bits, [samples, self.bitwidth])\n",
    "        self.x1_samples_bits = np.reshape(self.x1_bits, [samples, self.bitwidth])\n",
    "        self.x_samples_bits_dims = np.dstack([self.x0_samples_bits, self.x1_samples_bits])\n",
    "        self.y_samples_bits_dims = np.reshape(self.y_bits, [samples, self.bitwidth, 1])\n",
    "        self.batch_count = int(samples / batch_size)\n",
    "        self.x_all = np.split(self.x_samples_bits_dims, self.batch_count, axis=0)\n",
    "        self.y_all = np.split(self.y_samples_bits_dims, self.batch_count, axis=0)\n",
    "        train_batches = int(train_test_batches_split * self.batch_count)\n",
    "        test_batches = self.batch_count - train_batches\n",
    "        self.x_train = self.x_all[:train_batches - 1]\n",
    "        self.y_train = self.y_all[:train_batches - 1]\n",
    "        self.x_test = self.x_all[train_batches:]\n",
    "        self.y_test = self.y_all[train_batches:]\n",
    "        logging.info(\"Training set size\")\n",
    "        print(\"Training set size:\")\n",
    "        self.print_batch_dims(name=\"x_train\", var=self.x_train)\n",
    "        self.print_batch_dims(name=\"y_train\", var=self.y_train)\n",
    "        print(\"Test set size:\")\n",
    "        self.print_batch_dims(name=\"x_test\", var=self.x_test)\n",
    "        self.print_batch_dims(name=\"y_test\", var=self.y_test)\n",
    "        \n",
    "    def gen_x(self, samples, dtype):\n",
    "        # Would be nice to generate x without replacement however it is too expensive at 32-bit.\n",
    "        x_init_uint8 = lambda : np.reshape(np.random.choice(max_uint8, samples,\n",
    "                                                 replace=True).astype(np.uint8), [samples, 1])\n",
    "        x_init_uint16 = lambda : np.reshape(np.random.choice(max_uint16, samples,\n",
    "                                                 replace=True).astype(np.uint16), [samples, 1])\n",
    "        x_init_uint32 = lambda : np.reshape(np.random.choice(max_uint32, samples,\n",
    "                                                 replace=True).astype(np.uint32), [samples, 1])\n",
    "        if dtype == \"uint8\":\n",
    "            x0_uint = x_init_uint8()\n",
    "            x1_uint = x_init_uint8()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint16\":\n",
    "            x0_uint = x_init_uint16()\n",
    "            x1_uint = x_init_uint16()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint32\":\n",
    "            x0_uint = x_init_uint32()\n",
    "            x1_uint = x_init_uint32()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        return (x0_uint, x1_uint)\n",
    "        \n",
    "    def calc_y(self, x0_uint, x1_uint, dtype):\n",
    "        temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint8\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint8)\n",
    "        if dtype == \"uint16\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint16)\n",
    "        if dtype == \"uint32\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint32)\n",
    "        return y_uint\n",
    "        \n",
    "    def print_batch_dims(self, name, var):\n",
    "        print(name + \" batches : \" + str(len(var)))\n",
    "        print(name + \" batch shape: \" + str(var[0].shape))\n",
    "        \n",
    "        \n",
    "    def print_batch(self, batch_no):\n",
    "        print(self.x[batch_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr2inbits(x):\n",
    "    '''\n",
    "    Function for converting Unsigned bitwidth-bit integers to big endian binary representation.\n",
    "    Output is flipped so order is lsb to msb\n",
    "    '''\n",
    "    x_little_end = x.astype(x.dtype.newbyteorder(\"B\"))\n",
    "    x_little_end_uint8 = x_little_end.view(np.uint8)\n",
    "    x_bits = np.unpackbits(x_little_end_uint8)\n",
    "    x_bits_flipped = x_bits[::-1]\n",
    "    return x_bits_flipped\n",
    "def test_arr2inbits():\n",
    "    x_test = np.array([3,5], dtype=\"uint32\")\n",
    "    x_test_bits = arr2inbits(x_test)\n",
    "    assert (x_test_bits == np.array([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0\n",
    "                                    ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])).all()\n",
    "    x_test = np.array([3,5], dtype=\"uint8\")\n",
    "    x_test_bits = arr2inbits(x_test)\n",
    "    assert (x_test_bits == np.array([1,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0])).all()\n",
    "test_arr2inbits()\n",
    "def bits2arr(x, dtype):\n",
    "    x_bits_flipped = x[::-1]\n",
    "    x_int = np.packbits(x_bits_flipped)\n",
    "    if dtype == \"uint8\":\n",
    "        return x_int\n",
    "    if dtype == \"uint16\":\n",
    "        x_grouped_bytes = np.reshape(x_int, [int(x_int.shape[0] / 2), 2])\n",
    "        multiplier = np.array([2 ** 8, 1])\n",
    "    if dtype == \"uint32\":\n",
    "        x_grouped_bytes = np.reshape(x_int, [int(x_int.shape[0] / 4), 4])\n",
    "        multiplier = np.array([2 ** 24, 2 ** 16, 2 ** 8, 1])\n",
    "    x_weighted_grouped_bytes = multiplier * x_grouped_bytes\n",
    "    x_int_reduced = np.add.reduce(x_weighted_grouped_bytes, axis=1)\n",
    "    return x_int_reduced\n",
    "        \n",
    "def test_bits2arr():\n",
    "    x_test = np.array([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0\n",
    "                                    ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    x_test_int = bits2arr(x_test, \"uint32\")\n",
    "    assert(x_test_int == np.array([3,5])).all()\n",
    "    x_test = np.array([0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    x_test_int = bits2arr(x_test, \"uint32\")\n",
    "    assert(x_test_int == np.array([256])).all()\n",
    "test_bits2arr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:\n",
      "x_train batches : 7\n",
      "x_train batch shape: (16, 32, 2)\n",
      "y_train batches : 7\n",
      "y_train batch shape: (16, 32, 1)\n",
      "Test set size:\n",
      "x_test batches : 8\n",
      "x_test batch shape: (16, 32, 2)\n",
      "y_test batches : 8\n",
      "y_test batch shape: (16, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = SampleData(samples=samples, \n",
    "                        dtype=dtype, \n",
    "                        batch_size=batch_size, \n",
    "                        train_test_batches_split=train_test_batches_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Tensorflow RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RnnCell(object):\n",
    "    def __init__(self, hidden_neurons=16, bitwidth=32, use_tf_rnn_api=True):\n",
    "        # Tensorflow placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, bitwidth, 2], name=\"x\")\n",
    "        self.y = tf.placeholder(tf.float32 , [None, bitwidth, 1], name=\"y\")\n",
    "        self.initial_state = tf.placeholder(tf.float32 , [None, hidden_neurons], name=\"initial_state\")\n",
    "        \n",
    "        # Extract time series as list \n",
    "        self._x_series = tf.unstack(self.x, axis=1)\n",
    "        self._y_series = tf.unstack(self.y, axis=1)\n",
    "        \n",
    "        # Tensorflow weights and biases\n",
    "        self._weights, self._bias = {}, {}\n",
    "        \n",
    "        # Output layer parameters\n",
    "        self._weights[\"h_o\"] = tf.Variable(tf.random_uniform([hidden_neurons, 1], -0.1, 0.1),dtype=tf.float32, name=\"w_h_o\")\n",
    "        self._bias[\"o\"] = tf.Variable(tf.random_uniform([1], -0.1, 0.1), dtype=tf.float32, name=\"b_o\")\n",
    "        \n",
    "        # Manual static RNN implementation\n",
    "        if not use_tf_rnn_api:\n",
    "            # Input layer parameters\n",
    "            self._weights[\"i_h\"] = tf.Variable(\n",
    "                tf.random_uniform([2, hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"w_i_h\")\n",
    "\n",
    "            # Hidden layer parameters \n",
    "            self._weights[\"h_h\"] = tf.Variable(\n",
    "                tf.random_uniform([hidden_neurons, hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"w_h_h\")\n",
    "            self._bias[\"h\"] = tf.Variable(\n",
    "                tf.random_uniform([hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"b_h\")\n",
    "\n",
    "            h_0 = tf.Variable(np.zeros([1, hidden_neurons]),dtype=tf.float32)\n",
    "            h_0 = self.initial_state\n",
    "            self._h, self._logits_series = [], []\n",
    "            for current_input in self._x_series: \n",
    "                # Hidden layer activation is a function of current_inputs, previous hidden layer and bias. \n",
    "                temp_h_1 = tf.add(tf.matmul(current_input, self._weights[\"i_h\"]), self._bias[\"h\"])\n",
    "                h_1 = tf.nn.relu(tf.add(temp_h_1, tf.matmul(h_0, self._weights[\"h_h\"])))\n",
    "                # Output layer activation is a function of current hidden layer and bias\n",
    "                o_1_logit = tf.add(tf.matmul(h_1, self._weights[\"h_o\"]), self._bias[\"o\"])\n",
    "                # Previous hidden layer activation becomes the current hidden layer activation for\n",
    "                # the next timestep\n",
    "                self._logits_series.append(o_1_logit)\n",
    "                self._h.append(h_1)\n",
    "                h_0 = h_1\n",
    "        \n",
    "        # Tensorflow RNN API.  Can use static_rnn or dynamic_rnn in this case.\n",
    "        else:\n",
    "            cell = BasicRNNCell(hidden_neurons)\n",
    "            states, current_state = tf.nn.dynamic_rnn(cell, self.x, initial_state=self.initial_state)\n",
    "            states_series= tf.unstack(states, axis=1)\n",
    "            self._logits_series= [tf.add(tf.matmul(state, self._weights[\"h_o\"]), self._bias[\"o\"]) \n",
    "                                  for state in states_series]\n",
    "            \n",
    "        self.predictions_series = [tf.sigmoid(logits) for logits in self._logits_series]\n",
    "        predictions_labels = zip(self.predictions_series, self._y_series)\n",
    "        logits_labels = zip(self._logits_series, self._y_series)\n",
    "        self._losses = self.cost_func_logits(logits_labels)\n",
    "        self.total_loss = tf.reduce_mean(self._losses)\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(self.total_loss)\n",
    "    \n",
    "    def cost_func_logits(self, logits_labels):\n",
    "        losses = []\n",
    "        for logits, labels in logits_labels:\n",
    "            losses.append(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rnn = RnnCell(hidden_neurons=hidden_neurons, bitwidth=train_data.bitwidth, use_tf_rnn_api=use_tf_rnn_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(sess, rnn, x, y, initial_state):\n",
    "    total_loss_batch = []\n",
    "    predictions_batch = []\n",
    "    _y_series_batch = []\n",
    "    for batch_no in range(len(x)):\n",
    "        predictions, _y_series, total_loss, _ = sess.run(\n",
    "                        [rnn.predictions_series, \n",
    "                         rnn._y_series, \n",
    "                         rnn.total_loss, \n",
    "                         rnn.train_step],\n",
    "                        feed_dict={\n",
    "                            rnn.x: x[batch_no],\n",
    "                            rnn.y: y[batch_no],\n",
    "                            rnn.initial_state: initial_state\n",
    "                        })\n",
    "        total_loss_batch.append(total_loss)\n",
    "        predictions_batch += predictions\n",
    "        _y_series_batch += _y_series\n",
    "    return zip(predictions_batch, _y_series_batch, total_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_epoch(sess, rnn, x, y, initial_state):\n",
    "    total_loss_batch = []\n",
    "    predictions_batch = []\n",
    "    _y_series_batch = []\n",
    "    for batch_no in range(len(x)):\n",
    "        predictions, _y_series, total_loss = sess.run(\n",
    "                        [rnn.predictions_series, \n",
    "                         rnn._y_series, \n",
    "                         rnn.total_loss],\n",
    "                        feed_dict={\n",
    "                            rnn.x: x[batch_no],\n",
    "                            rnn.y: y[batch_no],\n",
    "                            rnn.initial_state: initial_state\n",
    "                        })\n",
    "        total_loss_batch.append(total_loss)\n",
    "        predictions_batch += predictions\n",
    "        _y_series_batch += _y_series\n",
    "    return zip(predictions_batch, _y_series_batch, total_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd1ac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_total_loss_series_all = []\n",
    "test_total_loss_series_all = []\n",
    "predictions_series_all = []\n",
    "_y_series_series_all = []\n",
    "init_state = np.zeros([batch_size, hidden_neurons])\n",
    "for epoch in range(epochs):\n",
    "    predictions_batch, _y_series_batch, train_total_loss_batch  = zip(*train_epoch(sess=sess, \n",
    "                                                                            rnn=my_rnn, \n",
    "                                                                            x=train_data.x_train, \n",
    "                                                                            y=train_data.y_train,\n",
    "                                                                            initial_state=init_state))\n",
    "    predictions_batch, _y_series_batch, test_total_loss_batch  = zip(*predict_epoch(sess=sess, \n",
    "                                                                            rnn=my_rnn, \n",
    "                                                                            x=train_data.x_test, \n",
    "                                                                            y=train_data.y_test,\n",
    "                                                                            initial_state=init_state))\n",
    "    train_total_loss_series_all.append(np.mean(train_total_loss_batch))\n",
    "    test_total_loss_series_all.append(np.mean(test_total_loss_batch))\n",
    "    plt.plot(range(len(train_total_loss_series_all)), train_total_loss_series_all, \"red\",\n",
    "             range(len(test_total_loss_series_all)), test_total_loss_series_all, \"blue\")\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculator Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNCalc(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dtype, rnn):\n",
    "        self.rnn = rnn\n",
    "        if dtype==\"uint8\":\n",
    "            self.max_val = 2 ** 8 - 1\n",
    "            self.dtype = dtype\n",
    "            self.np_dtype = np.uint8\n",
    "        if dtype==\"uint16\":\n",
    "            self.dtype = dtype\n",
    "            self.max_val = 2 ** 16 -1\n",
    "            self.np_dtype = np.uint16\n",
    "        if dtype==\"uint32\":\n",
    "            self.dtype = dtype\n",
    "            self.max_val = 2 ** 32 -1\n",
    "            self.np_dtype = np.uint32\n",
    "        \n",
    "    def _check_input(self, input_str: str):\n",
    "        x_unsafe = input(input_str)\n",
    "        try:\n",
    "            int(x_unsafe)\n",
    "        except:\n",
    "            print(\"Not an integer!\")\n",
    "            return\n",
    "        if int(x_unsafe) < 0: \n",
    "            print(\"Input must not be less than 0\")\n",
    "            return\n",
    "        if int(x_unsafe) > self.max_val: \n",
    "            print(\"Input must not be greater than {}\".format(self.max_val))\n",
    "            return\n",
    "        x_safe = int(x_unsafe)\n",
    "        return x_safe\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            x0 = self._check_input(\"Please input x0 in range 0 to {}\".format(self.max_val))\n",
    "            x1 = self._check_input(\"Please input x1 in range 0 to {}\".format(self.max_val))\n",
    "            if x0 is not None and x1 is not None:\n",
    "                x0_bits = arr2inbits(np.array((x0,), dtype=self.np_dtype))\n",
    "                x1_bits = arr2inbits(np.array((x1,), dtype=self.np_dtype))\n",
    "                x = np.dstack([x0_bits, x1_bits])\n",
    "                print(\"x0 int val: {}\".format(x0))\n",
    "                print(\"x0 bin val: {}\".format(x0_bits))\n",
    "                print(\"x0 int val: {}\".format(x1))\n",
    "                print(\"x0 bin val: {}\".format(x1_bits))\n",
    "                predictions= sess.run(\n",
    "                [self.rnn.predictions_series],\n",
    "                feed_dict={\n",
    "                    self.rnn.x: x,\n",
    "                    self.rnn.initial_state: np.zeros([1, hidden_neurons])\n",
    "                })\n",
    "                rounded_predictions = np.around(predictions)\n",
    "                predictions_cleaned = np.reshape(np.array(rounded_predictions[0]), [32]).astype(self.np_dtype)\n",
    "                print(\"y  int val: {} and correct answer is {}\"\n",
    "                      .format(bits2arr(predictions_cleaned, self.dtype)[0], x0 + x1))\n",
    "                print(\"y  bin val: {}\".format(predictions_cleaned))\n",
    "                #DO CALCULATION\n",
    "            user_input = input(\"Do you wish to perform a new calculation? (y/N)\")\n",
    "            if user_input not in \"Yy\" :\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_calc = RNNCalc(dtype, my_rnn)\n",
    "my_test_calc.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
