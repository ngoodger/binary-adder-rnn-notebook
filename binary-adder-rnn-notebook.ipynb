{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation of Binary addition\n",
    "\n",
    "## Contents\n",
    "1. Generate sample data\n",
    "2. Build Tensorflow RNN model\n",
    "3. Train model\n",
    "4. Calculator wrapper\n",
    "\n",
    "Can be configured in 32, 16, and 8-bit modes however since the binary addition operation generalizes perfectly for each time step the bitwidth doesn't make much difference. For the same reason very few training examples are required to train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "from IPython import display\n",
    "from tensorflow.contrib.rnn import BasicRNNCell\n",
    "\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "# Max unsigned integer values\n",
    "max_uint32 = 2 ** 32 - 1\n",
    "max_uint16 = 2 ** 16 - 1\n",
    "max_uint8 = 2 ** 8 - 1\n",
    "\n",
    "# RNN implementation\n",
    "# RNN can be implemented using tensorflow api for RNN or by manually unrolling sequence.\n",
    "use_tf_rnn_api = False\n",
    "\n",
    "# Adder datatype\n",
    "# supports\n",
    "# uint32, uint16 and uint8\n",
    "dtype = \"uint32\"\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 3e-3\n",
    "hidden_neurons = 16\n",
    "\n",
    "# Training data\n",
    "samples = 256\n",
    "train_validation_batches_split = 0.5\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampleData(object):\n",
    "\n",
    "    def __init__(self, samples, dtype, batch_size, train_validation_batches_split):\n",
    "        valid_dtypes = {\"uint8\", \"uint16\", \"uint32\"}\n",
    "        if not dtype in valid_dtypes: raise ValueError(\"input dtype not in valid dtypes\")\n",
    "        if (samples % batch_size != 0): raise ValueError(\"samples must be multiple of batch_size\")\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        \n",
    "        if dtype == \"uint8\":\n",
    "            self.bitwidth = 8\n",
    "        if dtype == \"uint16\":\n",
    "            self.bitwidth = 16\n",
    "        if dtype == \"uint32\":\n",
    "            self.bitwidth = 32\n",
    "        \n",
    "        self.x0_uint, self.x1_uint = self.gen_x(samples, dtype)\n",
    "        self.y_uint = self.calc_y(self.x0_uint, self.x1_uint, dtype)\n",
    "        \n",
    "        self.x0_bits = arr2inbits(self.x0_uint)\n",
    "        self.x1_bits = arr2inbits(self.x1_uint)\n",
    "        self.y_bits = arr2inbits(self.y_uint)\n",
    "        self.x0_samples_bits = np.reshape(self.x0_bits, [samples, self.bitwidth])\n",
    "        self.x1_samples_bits = np.reshape(self.x1_bits, [samples, self.bitwidth])\n",
    "        self.x_samples_bits_dims = np.dstack([self.x0_samples_bits, self.x1_samples_bits])\n",
    "        self.y_samples_bits_dims = np.reshape(self.y_bits, [samples, self.bitwidth, 1])\n",
    "        self.batch_count = int(samples / batch_size)\n",
    "        self.x_all = np.split(self.x_samples_bits_dims, self.batch_count, axis=0)\n",
    "        self.y_all = np.split(self.y_samples_bits_dims, self.batch_count, axis=0)\n",
    "        train_batches = int(train_validation_batches_split * self.batch_count)\n",
    "        validation_batches = self.batch_count - train_batches\n",
    "        self.x_train = self.x_all[:train_batches - 1]\n",
    "        self.y_train = self.y_all[:train_batches - 1]\n",
    "        self.x_validation = self.x_all[train_batches:]\n",
    "        self.y_validation = self.y_all[train_batches:]\n",
    "        logging.info(\"Training set size\")\n",
    "        print(\"Training set size:\")\n",
    "        self.print_batch_dims(name=\"x_train\", var=self.x_train)\n",
    "        self.print_batch_dims(name=\"y_train\", var=self.y_train)\n",
    "        print(\"Validation set size:\")\n",
    "        self.print_batch_dims(name=\"x_validation\", var=self.x_validation)\n",
    "        self.print_batch_dims(name=\"y_validation\", var=self.y_validation)\n",
    "        \n",
    "    def gen_x(self, samples, dtype):\n",
    "        # Would be nice to generate x without replacement however it is too expensive at 32-bit.\n",
    "        x_init_uint8 = lambda : np.reshape(np.random.choice(max_uint8, samples,\n",
    "                                                 replace=True).astype(np.uint8), [samples, 1])\n",
    "        x_init_uint16 = lambda : np.reshape(np.random.choice(max_uint16, samples,\n",
    "                                                 replace=True).astype(np.uint16), [samples, 1])\n",
    "        x_init_uint32 = lambda : np.reshape(np.random.choice(max_uint32, samples,\n",
    "                                                 replace=True).astype(np.uint32), [samples, 1])\n",
    "        if dtype == \"uint8\":\n",
    "            x0_uint = x_init_uint8()\n",
    "            x1_uint = x_init_uint8()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint16\":\n",
    "            x0_uint = x_init_uint16()\n",
    "            x1_uint = x_init_uint16()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint32\":\n",
    "            x0_uint = x_init_uint32()\n",
    "            x1_uint = x_init_uint32()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        return (x0_uint, x1_uint)\n",
    "        \n",
    "    def calc_y(self, x0_uint, x1_uint, dtype):\n",
    "        temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint8\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint8)\n",
    "        if dtype == \"uint16\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint16)\n",
    "        if dtype == \"uint32\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint32)\n",
    "        return y_uint\n",
    "        \n",
    "    def print_batch_dims(self, name, var):\n",
    "        print(name + \" batches : \" + str(len(var)))\n",
    "        print(name + \" batch shape: \" + str(var[0].shape))\n",
    "        \n",
    "        \n",
    "    def print_batch(self, batch_no):\n",
    "        print(self.x[batch_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr2inbits(x):\n",
    "    '''\n",
    "    Function for converting Unsigned bitwidth-bit integers to big endian binary representation.\n",
    "    Output is flipped so order is lsb to msb\n",
    "    '''\n",
    "    x_little_end = x.astype(x.dtype.newbyteorder(\"B\"))\n",
    "    x_little_end_uint8 = x_little_end.view(np.uint8)\n",
    "    x_bits = np.unpackbits(x_little_end_uint8)\n",
    "    x_bits_flipped = x_bits[::-1]\n",
    "    return x_bits_flipped\n",
    "def test_arr2inbits():\n",
    "    x_test = np.array([3,5], dtype=\"uint32\")\n",
    "    x_test_bits = arr2inbits(x_test)\n",
    "    assert (x_test_bits == np.array([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0\n",
    "                                    ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])).all()\n",
    "    x_test = np.array([3,5], dtype=\"uint8\")\n",
    "    x_test_bits = arr2inbits(x_test)\n",
    "    assert (x_test_bits == np.array([1,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0])).all()\n",
    "test_arr2inbits()\n",
    "def bits2arr(x, dtype):\n",
    "    x_bits_flipped = x[::-1]\n",
    "    x_int = np.packbits(x_bits_flipped)\n",
    "    if dtype == \"uint8\":\n",
    "        return x_int\n",
    "    if dtype == \"uint16\":\n",
    "        x_grouped_bytes = np.reshape(x_int, [int(x_int.shape[0] / 2), 2])\n",
    "        multiplier = np.array([2 ** 8, 1])\n",
    "    if dtype == \"uint32\":\n",
    "        x_grouped_bytes = np.reshape(x_int, [int(x_int.shape[0] / 4), 4])\n",
    "        multiplier = np.array([2 ** 24, 2 ** 16, 2 ** 8, 1])\n",
    "    x_weighted_grouped_bytes = multiplier * x_grouped_bytes\n",
    "    x_int_reduced = np.add.reduce(x_weighted_grouped_bytes, axis=1)\n",
    "    return x_int_reduced\n",
    "        \n",
    "def test_bits2arr():\n",
    "    x_test = np.array([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0\n",
    "                                    ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    x_test_int = bits2arr(x_test, \"uint32\")\n",
    "    assert(x_test_int == np.array([3,5])).all()\n",
    "    x_test = np.array([0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    x_test_int = bits2arr(x_test, \"uint32\")\n",
    "    assert(x_test_int == np.array([256])).all()\n",
    "test_bits2arr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:\n",
      "x_train batches : 7\n",
      "x_train batch shape: (16, 32, 2)\n",
      "y_train batches : 7\n",
      "y_train batch shape: (16, 32, 1)\n",
      "Validation set size:\n",
      "x_validation batches : 8\n",
      "x_validation batch shape: (16, 32, 2)\n",
      "y_validation batches : 8\n",
      "y_validation batch shape: (16, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = SampleData(samples=samples, \n",
    "                        dtype=dtype, \n",
    "                        batch_size=batch_size, \n",
    "                        train_validation_batches_split=train_validation_batches_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Tensorflow RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RnnCell(object):\n",
    "    def __init__(self, hidden_neurons=16, bitwidth=32, use_tf_rnn_api=True):\n",
    "        # Tensorflow placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, bitwidth, 2], name=\"x\")\n",
    "        self.y = tf.placeholder(tf.float32 , [None, bitwidth, 1], name=\"y\")\n",
    "        self.initial_state = tf.placeholder(tf.float32 , [None, hidden_neurons], name=\"initial_state\")\n",
    "        \n",
    "        # Extract time series as list \n",
    "        self._x_series = tf.unstack(self.x, axis=1)\n",
    "        self._y_series = tf.unstack(self.y, axis=1)\n",
    "        \n",
    "        # Tensorflow weights and biases\n",
    "        self._weights, self._bias = {}, {}\n",
    "        \n",
    "        # Output layer parameters\n",
    "        self._weights[\"h_o\"] = tf.Variable(tf.random_uniform([hidden_neurons, 1], -0.1, 0.1),dtype=tf.float32, name=\"w_h_o\")\n",
    "        self._bias[\"o\"] = tf.Variable(tf.random_uniform([1], -0.1, 0.1), dtype=tf.float32, name=\"b_o\")\n",
    "        \n",
    "        # Manual static RNN implementation\n",
    "        if not use_tf_rnn_api:\n",
    "            # Input layer parameters\n",
    "            self._weights[\"i_h\"] = tf.Variable(\n",
    "                tf.random_uniform([2, hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"w_i_h\")\n",
    "\n",
    "            # Hidden layer parameters \n",
    "            self._weights[\"h_h\"] = tf.Variable(\n",
    "                tf.random_uniform([hidden_neurons, hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"w_h_h\")\n",
    "            self._bias[\"h\"] = tf.Variable(\n",
    "                tf.random_uniform([hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"b_h\")\n",
    "\n",
    "            h_0 = tf.Variable(np.zeros([1, hidden_neurons]),dtype=tf.float32)\n",
    "            h_0 = self.initial_state\n",
    "            self._h, self._logits_series = [], []\n",
    "            for current_input in self._x_series: \n",
    "                # Hidden layer activation is a function of current_inputs, previous hidden layer and bias. \n",
    "                temp_h_1 = tf.add(tf.matmul(current_input, self._weights[\"i_h\"]), self._bias[\"h\"])\n",
    "                h_1 = tf.nn.relu(tf.add(temp_h_1, tf.matmul(h_0, self._weights[\"h_h\"])))\n",
    "                # Output layer activation is a function of current hidden layer and bias\n",
    "                o_1_logit = tf.add(tf.matmul(h_1, self._weights[\"h_o\"]), self._bias[\"o\"])\n",
    "                # Previous hidden layer activation becomes the current hidden layer activation for\n",
    "                # the next timestep\n",
    "                self._logits_series.append(o_1_logit)\n",
    "                self._h.append(h_1)\n",
    "                h_0 = h_1\n",
    "        \n",
    "        # Tensorflow RNN API.  Can use static_rnn or dynamic_rnn in this case.\n",
    "        else:\n",
    "            cell = BasicRNNCell(hidden_neurons)\n",
    "            states, current_state = tf.nn.dynamic_rnn(cell, self.x, initial_state=self.initial_state)\n",
    "            states_series= tf.unstack(states, axis=1)\n",
    "            self._logits_series= [tf.add(tf.matmul(state, self._weights[\"h_o\"]), self._bias[\"o\"]) \n",
    "                                  for state in states_series]\n",
    "            \n",
    "        self.predictions_series = [tf.sigmoid(logits) for logits in self._logits_series]\n",
    "        predictions_labels = zip(self.predictions_series, self._y_series)\n",
    "        logits_labels = zip(self._logits_series, self._y_series)\n",
    "        self._losses = self.cost_func_logits(logits_labels)\n",
    "        self.total_loss = tf.reduce_mean(self._losses)\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(self.total_loss)\n",
    "    \n",
    "    def cost_func_logits(self, logits_labels):\n",
    "        losses = []\n",
    "        for logits, labels in logits_labels:\n",
    "            losses.append(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rnn = RnnCell(hidden_neurons=hidden_neurons, bitwidth=train_data.bitwidth, use_tf_rnn_api=use_tf_rnn_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(sess, rnn, x, y, initial_state):\n",
    "    total_loss_batch = []\n",
    "    predictions_batch = []\n",
    "    _y_series_batch = []\n",
    "    for batch_no in range(len(x)):\n",
    "        predictions, _y_series, total_loss, _ = sess.run(\n",
    "                        [rnn.predictions_series, \n",
    "                         rnn._y_series, \n",
    "                         rnn.total_loss, \n",
    "                         rnn.train_step],\n",
    "                        feed_dict={\n",
    "                            rnn.x: x[batch_no],\n",
    "                            rnn.y: y[batch_no],\n",
    "                            rnn.initial_state: initial_state\n",
    "                        })\n",
    "        total_loss_batch.append(total_loss)\n",
    "        predictions_batch += predictions\n",
    "        _y_series_batch += _y_series\n",
    "    return zip(predictions_batch, _y_series_batch, total_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_epoch(sess, rnn, x, y, initial_state):\n",
    "    total_loss_batch = []\n",
    "    predictions_batch = []\n",
    "    _y_series_batch = []\n",
    "    for batch_no in range(len(x)):\n",
    "        predictions, _y_series, total_loss = sess.run(\n",
    "                        [rnn.predictions_series, \n",
    "                         rnn._y_series, \n",
    "                         rnn.total_loss],\n",
    "                        feed_dict={\n",
    "                            rnn.x: x[batch_no],\n",
    "                            rnn.y: y[batch_no],\n",
    "                            rnn.initial_state: initial_state\n",
    "                        })\n",
    "        total_loss_batch.append(total_loss)\n",
    "        predictions_batch += predictions\n",
    "        _y_series_batch += _y_series\n",
    "    return zip(predictions_batch, _y_series_batch, total_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkoEU7iIiGUjhLiKSgf4fiakRgEyprpgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b0ac470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_total_loss_series_all = []\n",
    "validation_total_loss_series_all = []\n",
    "predictions_series_all = []\n",
    "_y_series_series_all = []\n",
    "init_state = np.zeros([batch_size, hidden_neurons])\n",
    "for epoch in range(epochs):\n",
    "    predictions_batch, _y_series_batch, train_total_loss_batch  = zip(*train_epoch(sess=sess, \n",
    "                                                                            rnn=my_rnn, \n",
    "                                                                            x=train_data.x_train, \n",
    "                                                                            y=train_data.y_train,\n",
    "                                                                            initial_state=init_state))\n",
    "    predictions_batch, _y_series_batch, validation_total_loss_batch  = zip(*predict_epoch(sess=sess, \n",
    "                                                                            rnn=my_rnn, \n",
    "                                                                            x=train_data.x_validation, \n",
    "                                                                            y=train_data.y_validation,\n",
    "                                                                            initial_state=init_state))\n",
    "    train_total_loss_series_all.append(np.mean(train_total_loss_batch))\n",
    "    validation_total_loss_series_all.append(np.mean(validation_total_loss_batch))\n",
    "    plt.plot(range(len(train_total_loss_series_all)), train_total_loss_series_all, \"red\",\n",
    "             range(len(validation_total_loss_series_all)), validation_total_loss_series_all, \"blue\")\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculator Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNCalc(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dtype, rnn):\n",
    "        self.rnn = rnn\n",
    "        if dtype==\"uint8\":\n",
    "            self.max_val = 2 ** 8 - 1\n",
    "            self.dtype = dtype\n",
    "            self.np_dtype = np.uint8\n",
    "        if dtype==\"uint16\":\n",
    "            self.dtype = dtype\n",
    "            self.max_val = 2 ** 16 -1\n",
    "            self.np_dtype = np.uint16\n",
    "        if dtype==\"uint32\":\n",
    "            self.dtype = dtype\n",
    "            self.max_val = 2 ** 32 -1\n",
    "            self.np_dtype = np.uint32\n",
    "        \n",
    "    def _check_input(self, input_str: str):\n",
    "        x_unsafe = input(input_str)\n",
    "        try:\n",
    "            int(x_unsafe)\n",
    "        except:\n",
    "            print(\"Not an integer!\")\n",
    "            return\n",
    "        if int(x_unsafe) < 0: \n",
    "            print(\"Input must not be less than 0\")\n",
    "            return\n",
    "        if int(x_unsafe) > self.max_val: \n",
    "            print(\"Input must not be greater than {}\".format(self.max_val))\n",
    "            return\n",
    "        x_safe = int(x_unsafe)\n",
    "        return x_safe\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            x0 = self._check_input(\"Please input x0 in range 0 to {}\".format(self.max_val))\n",
    "            x1 = self._check_input(\"Please input x1 in range 0 to {}\".format(self.max_val))\n",
    "            if x0 is not None and x1 is not None:\n",
    "                x0_bits = arr2inbits(np.array((x0,), dtype=self.np_dtype))\n",
    "                x1_bits = arr2inbits(np.array((x1,), dtype=self.np_dtype))\n",
    "                x = np.dstack([x0_bits, x1_bits])\n",
    "                print(\"x0 int val: {}\".format(x0))\n",
    "                print(\"x0 bin val: {}\".format(x0_bits))\n",
    "                print(\"x0 int val: {}\".format(x1))\n",
    "                print(\"x0 bin val: {}\".format(x1_bits))\n",
    "                predictions= sess.run(\n",
    "                [self.rnn.predictions_series],\n",
    "                feed_dict={\n",
    "                    self.rnn.x: x,\n",
    "                    self.rnn.initial_state: np.zeros([1, hidden_neurons])\n",
    "                })\n",
    "                rounded_predictions = np.around(predictions)\n",
    "                predictions_cleaned = np.reshape(np.array(rounded_predictions[0]), [32]).astype(self.np_dtype)\n",
    "                print(\"y  int val: {} and correct answer is {}\"\n",
    "                      .format(bits2arr(predictions_cleaned, self.dtype)[0], x0 + x1))\n",
    "                print(\"y  bin val: {}\".format(predictions_cleaned))\n",
    "                #DO CALCULATION\n",
    "            user_input = input(\"Do you wish to perform a new calculation? (y/N)\")\n",
    "            if user_input not in \"Yy\" :\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input x0 in range 0 to 429496729545246\n",
      "Please input x1 in range 0 to 4294967295765747\n",
      "x0 int val: 45246\n",
      "x0 bin val: [0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "x0 int val: 765747\n",
      "x0 bin val: [1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y  int val: 810993 and correct answer is 810993\n",
      "y  bin val: [1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "my_test_calc = RNNCalc(dtype, my_rnn)\n",
    "my_test_calc.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
