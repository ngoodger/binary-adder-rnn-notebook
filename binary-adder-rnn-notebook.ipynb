{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation of Binary addition\n",
    "\n",
    "## Contents\n",
    "1. Generate sample data\n",
    "2. Build Tensorflow RNN model\n",
    "3. Train model\n",
    "4. Calculator wrapper\n",
    "\n",
    "Can be configured in 32, 16, and 8-bit modes however since the binary addition operation generalizes perfectly for each time step the bitwidth doesn't make much difference. For the same reason very few training examples are required to train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%config IPCompleter.greedy=True\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "from IPython import display\n",
    "%matplotlib inline  \n",
    "\n",
    "# Adder datatype\n",
    "# supports\n",
    "# uint32, uint16 and uint8\n",
    "dtype = \"uint32\"\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 3e-3\n",
    "hidden_neurons = 16\n",
    "\n",
    "# Training data\n",
    "samples = 256\n",
    "train_test_batches_split = 0.5\n",
    "batch_size = 16\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampleData(object):\n",
    "\n",
    "    def __init__(self, samples, dtype, batch_size, train_test_batches_split):\n",
    "        valid_dtypes = {\"uint8\", \"uint16\", \"uint32\"}\n",
    "        if not dtype in valid_dtypes: raise ValueError(\"input dtype not in valid dtypes\")\n",
    "        if (samples % batch_size != 0): raise ValueError(\"samples must be multiple of batch_size\")\n",
    "        max_uint32 = 2 ** 32 - 1\n",
    "        max_uint16 = 2 ** 16 - 1\n",
    "        max_uint8 = 2 ** 8 - 1\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        \n",
    "        if dtype == \"uint8\":\n",
    "            self.bitwidth = 8\n",
    "        if dtype == \"uint16\":\n",
    "            self.bitwidth = 16\n",
    "        if dtype == \"uint32\":\n",
    "            self.bitwidth = 32\n",
    "        \n",
    "        self.x0_uint, self.x1_uint = self.gen_x(samples, dtype)\n",
    "        self.y_uint = self.calc_y(self.x0_uint, self.x1_uint, dtype)\n",
    "        \n",
    "        self.x0_bits = arr2inbits(self.x0_uint)\n",
    "        self.x1_bits = arr2inbits(self.x1_uint)\n",
    "        self.y_bits = arr2inbits(self.y_uint)\n",
    "        self.x0_samples_bits = np.reshape(self.x0_bits, [samples, self.bitwidth])\n",
    "        self.x1_samples_bits = np.reshape(self.x1_bits, [samples, self.bitwidth])\n",
    "        self.x_samples_bits_dims = np.dstack([self.x0_samples_bits, self.x1_samples_bits])\n",
    "        self.y_samples_bits_dims = np.reshape(self.y_bits, [samples, self.bitwidth, 1])\n",
    "        self.batch_count = int(samples / batch_size)\n",
    "        self.x_all = np.split(self.x_samples_bits_dims, self.batch_count, axis=0)\n",
    "        self.y_all = np.split(self.y_samples_bits_dims, self.batch_count, axis=0)\n",
    "        train_batches = int(train_test_batches_split * self.batch_count)\n",
    "        test_batches = self.batch_count - train_batches\n",
    "        self.x_train = self.x_all[:train_batches - 1]\n",
    "        self.y_train = self.y_all[:train_batches - 1]\n",
    "        self.x_test = self.x_all[train_batches:]\n",
    "        self.y_test = self.y_all[train_batches:]\n",
    "        logging.info(\"Training set size\")\n",
    "        print(\"Training set size:\")\n",
    "        self.print_batch_dims(name=\"x_train\", var=self.x_train)\n",
    "        self.print_batch_dims(name=\"y_train\", var=self.y_train)\n",
    "        print(\"Test set size:\")\n",
    "        self.print_batch_dims(name=\"x_test\", var=self.x_test)\n",
    "        self.print_batch_dims(name=\"y_test\", var=self.y_test)\n",
    "        \n",
    "    def gen_x(self, samples, dtype):\n",
    "        # Would be nice to generate x without replacement however it is too expensive at 32-bit.\n",
    "        x_init_uint8 = lambda : np.reshape(np.random.choice(max_uint8, samples,\n",
    "                                                 replace=True).astype(np.uint8), [samples, 1])\n",
    "        x_init_uint16 = lambda : np.reshape(np.random.choice(max_uint16, samples,\n",
    "                                                 replace=True).astype(np.uint16), [samples, 1])\n",
    "        x_init_uint32 = lambda : np.reshape(np.random.choice(max_uint32, samples,\n",
    "                                                 replace=True).astype(np.uint32), [samples, 1])\n",
    "        if dtype == \"uint8\":\n",
    "            x0_uint = x_init_uint8()\n",
    "            x1_uint = x_init_uint8()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint16\":\n",
    "            x0_uint = x_init_uint16()\n",
    "            x1_uint = x_init_uint16()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint32\":\n",
    "            x0_uint = x_init_uint32()\n",
    "            x1_uint = x_init_uint32()\n",
    "            temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        return (x0_uint, x1_uint)\n",
    "        \n",
    "    def calc_y(self, x0_uint, x1_uint, dtype):\n",
    "        temp_x = np.hstack([x0_uint, x1_uint])\n",
    "        if dtype == \"uint8\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint8)\n",
    "        if dtype == \"uint16\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint16)\n",
    "        if dtype == \"uint32\":\n",
    "            y_uint = np.sum(temp_x, axis=1, dtype=np.uint32)\n",
    "        return y_uint\n",
    "        \n",
    "    def print_batch_dims(self, name, var):\n",
    "        print(name + \" batches : \" + str(len(var)))\n",
    "        print(name + \" batch shape: \" + str(var[0].shape))\n",
    "        \n",
    "        \n",
    "    def print_batch(self, batch_no):\n",
    "        print(self.x[batch_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr2inbits(x):\n",
    "    '''\n",
    "    Function for converting Unsigned bitwidth-bit integers to big endian binary representation.\n",
    "    Output is flipped so order is lsb to msb\n",
    "    '''\n",
    "    x_little_end = x.astype(x.dtype.newbyteorder(\"B\"))\n",
    "    x_little_end_uint8 = x_little_end.view(np.uint8)\n",
    "    x_bits = np.unpackbits(x_little_end_uint8)\n",
    "    x_bits_flipped = x_bits[::-1]\n",
    "    return x_bits_flipped\n",
    "def test_arr2inbits():\n",
    "    x_test = np.array([3,5], dtype=\"uint32\")\n",
    "    x_test_bits = arr2inbits(x_test)\n",
    "    assert (x_test_bits == np.array([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0\n",
    "                                    ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])).all()\n",
    "    x_test = np.array([3,5], dtype=\"uint8\")\n",
    "    x_test_bits = arr2inbits(x_test)\n",
    "    assert (x_test_bits == np.array([1,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0])).all()\n",
    "test_arr2inbits()\n",
    "def bits2arr(x, dtype):\n",
    "    x_bits_flipped = x[::-1]\n",
    "    x_int = np.packbits(x_bits_flipped)\n",
    "    if dtype == \"uint8\":\n",
    "        return x_int\n",
    "    if dtype == \"uint16\":\n",
    "        x_grouped_bytes = np.reshape(x_int, [int(x_int.shape[0] / 2), 2])\n",
    "        multiplier = np.array([2 ** 8, 1])\n",
    "    if dtype == \"uint32\":\n",
    "        x_grouped_bytes = np.reshape(x_int, [int(x_int.shape[0] / 4), 4])\n",
    "        multiplier = np.array([2 ** 24, 2 ** 16, 2 ** 8, 1])\n",
    "    x_weighted_grouped_bytes = multiplier * x_grouped_bytes\n",
    "    x_int_reduced = np.add.reduce(x_weighted_grouped_bytes, axis=1)\n",
    "    return x_int_reduced\n",
    "        \n",
    "def test_bits2arr():\n",
    "    x_test = np.array([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0\n",
    "                                    ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    x_test_int = bits2arr(x_test, \"uint32\")\n",
    "    assert(x_test_int == np.array([3,5])).all()\n",
    "    x_test = np.array([0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    x_test_int = bits2arr(x_test, \"uint32\")\n",
    "    assert(x_test_int == np.array([256])).all()\n",
    "test_bits2arr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:\n",
      "x_train batches : 7\n",
      "x_train batch shape: (16, 32, 2)\n",
      "y_train batches : 7\n",
      "y_train batch shape: (16, 32, 1)\n",
      "Test set size:\n",
      "x_test batches : 8\n",
      "x_test batch shape: (16, 32, 2)\n",
      "y_test batches : 8\n",
      "y_test batch shape: (16, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = SampleData(samples=samples, \n",
    "                        dtype=dtype, \n",
    "                        batch_size=batch_size, \n",
    "                        train_test_batches_split=train_test_batches_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Tensorflow RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RnnCell(object):\n",
    "    def __init__(self, hidden_neurons=16, bitwidth=32):\n",
    "        # Tensorflow placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [None, bitwidth, 2], name=\"x\")\n",
    "        self.y = tf.placeholder(tf.float32 , [None, bitwidth, 1], name=\"y\")\n",
    "        self.initial_state = tf.placeholder(tf.float32 , [1, hidden_neurons], name=\"initial_state\")\n",
    "        \n",
    "        # Tensorflow weights and biases\n",
    "        self._weights, self._bias = {}, {}\n",
    "        \n",
    "        # Input layer parameters\n",
    "        self._weights[\"i_h\"] = tf.Variable(\n",
    "            tf.random_uniform([2, hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"w_i_h\")\n",
    "\n",
    "        # Hidden layer parameters \n",
    "        self._weights[\"h_h\"] = tf.Variable(\n",
    "            tf.random_uniform([hidden_neurons, hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"w_h_h\")\n",
    "        self._bias[\"h\"] = tf.Variable(\n",
    "            tf.random_uniform([hidden_neurons], -0.1, 0.1), dtype=tf.float32, name=\"b_h\")\n",
    "\n",
    "        # Output layer parameters\n",
    "        self._weights[\"h_o\"] = tf.Variable(tf.random_uniform([hidden_neurons, 1], -0.1, 0.1),dtype=tf.float32, name=\"w_h_o\")\n",
    "        self._bias[\"o\"] = tf.Variable(tf.random_uniform([1], -0.1, 0.1), dtype=tf.float32, name=\"b_o\")\n",
    "        # Extract time series as list \n",
    "        self._x_series = tf.unstack(self.x, axis=1)\n",
    "        self._y_series = tf.unstack(self.y, axis=1)\n",
    "        \n",
    "        h_0 = tf.Variable(np.zeros([1, hidden_neurons]),dtype=tf.float32)\n",
    "        h_0 = self.initial_state\n",
    "        self._h, self._logits_series = [], []\n",
    "        for current_input in self._x_series: \n",
    "            # Hidden layer activation is a function of current_inputs, previous hidden layer and bias. \n",
    "            temp_h_1 = tf.add(tf.matmul(current_input, self._weights[\"i_h\"]), self._bias[\"h\"])\n",
    "            h_1 = tf.nn.relu(tf.add(temp_h_1, tf.matmul(h_0, self._weights[\"h_h\"])))\n",
    "            # Output layer activation is a function of current hidden layer and bias\n",
    "            o_1_logit = tf.add(tf.matmul(h_1, self._weights[\"h_o\"]), self._bias[\"o\"])\n",
    "            # Previous hidden layer activation becomes the current hidden layer activation for\n",
    "            # the next timestep\n",
    "            self._logits_series.append(o_1_logit)\n",
    "            self._h.append(h_1)\n",
    "            h_0 = h_1\n",
    "        self.predictions_series = [tf.sigmoid(logits) for logits in self._logits_series]\n",
    "        predictions_labels = zip(self.predictions_series, self._y_series)\n",
    "        logits_labels = zip(self._logits_series, self._y_series)\n",
    "        self._losses = self.cost_func_logits(logits_labels)\n",
    "        self.total_loss = tf.reduce_mean(self._losses)\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(self.total_loss)\n",
    "    \n",
    "    def cost_func_logits(self, logits_labels):\n",
    "        losses = []\n",
    "        for logits, labels in logits_labels:\n",
    "            losses.append(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rnn = RnnCell(hidden_neurons=hidden_neurons, bitwidth=train_data.bitwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(sess, rnn, x, y, initial_state):\n",
    "    total_loss_batch = []\n",
    "    predictions_batch = []\n",
    "    _y_series_batch = []\n",
    "    for batch_no in range(len(x)):\n",
    "        predictions, _y_series, total_loss, _ = sess.run(\n",
    "                        [rnn.predictions_series, \n",
    "                         rnn._y_series, \n",
    "                         rnn.total_loss, \n",
    "                         rnn.train_step],\n",
    "                        feed_dict={\n",
    "                            rnn.x: x[batch_no],\n",
    "                            rnn.y: y[batch_no],\n",
    "                            rnn.initial_state: initial_state\n",
    "                        })\n",
    "        total_loss_batch.append(total_loss)\n",
    "        predictions_batch += predictions\n",
    "        _y_series_batch += _y_series\n",
    "    return zip(predictions_batch, _y_series_batch, total_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_epoch(sess, rnn, x, y, initial_state):\n",
    "    total_loss_batch = []\n",
    "    predictions_batch = []\n",
    "    _y_series_batch = []\n",
    "    for batch_no in range(len(x)):\n",
    "        predictions, _y_series, total_loss = sess.run(\n",
    "                        [rnn.predictions_series, \n",
    "                         rnn._y_series, \n",
    "                         rnn.total_loss],\n",
    "                        feed_dict={\n",
    "                            rnn.x: x[batch_no],\n",
    "                            rnn.y: y[batch_no],\n",
    "                            rnn.initial_state: initial_state\n",
    "                        })\n",
    "        total_loss_batch.append(total_loss)\n",
    "        predictions_batch += predictions\n",
    "        _y_series_batch += _y_series\n",
    "    return zip(predictions_batch, _y_series_batch, total_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npHIXEUlCKncRkST0/92D4k//n98zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125a4e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_total_loss_series_all = []\n",
    "test_total_loss_series_all = []\n",
    "predictions_series_all = []\n",
    "_y_series_series_all = []\n",
    "for epoch in range(epochs):\n",
    "    predictions_batch, _y_series_batch, train_total_loss_batch  = zip(*train_epoch(sess=sess, \n",
    "                                                                            rnn=my_rnn, \n",
    "                                                                            x=train_data.x_train, \n",
    "                                                                            y=train_data.y_train,\n",
    "                                                                            initial_state=np.zeros([1, hidden_neurons])))\n",
    "    predictions_batch, _y_series_batch, test_total_loss_batch  = zip(*predict_epoch(sess=sess, \n",
    "                                                                            rnn=my_rnn, \n",
    "                                                                            x=train_data.x_test, \n",
    "                                                                            y=train_data.y_test,\n",
    "                                                                            initial_state=np.zeros([1, hidden_neurons])))\n",
    "    train_total_loss_series_all.append(np.mean(train_total_loss_batch))\n",
    "    test_total_loss_series_all.append(np.mean(test_total_loss_batch))\n",
    "    plt.plot(range(len(train_total_loss_series_all)), train_total_loss_series_all, \"red\",\n",
    "             range(len(test_total_loss_series_all)), test_total_loss_series_all, \"blue\")\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculator Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCalc(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dtype, rnn):\n",
    "        self.rnn = rnn\n",
    "        if dtype==\"uint8\":\n",
    "            self.max_val = 2 ** 8 - 1\n",
    "            self.dtype = dtype\n",
    "            self.np_dtype = np.uint8\n",
    "        if dtype==\"uint16\":\n",
    "            self.dtype = dtype\n",
    "            self.max_val = 2 ** 16 -1\n",
    "            self.np_dtype = np.uint16\n",
    "        if dtype==\"uint32\":\n",
    "            self.dtype = dtype\n",
    "            self.max_val = 2 ** 32 -1\n",
    "            self.np_dtype = np.uint32\n",
    "        \n",
    "    def _check_input(self, input_str: str):\n",
    "        x_unsafe = input(input_str)\n",
    "        try:\n",
    "            int(x_unsafe)\n",
    "        except:\n",
    "            print(\"Not an integer!\")\n",
    "            return\n",
    "        if int(x_unsafe) < 0: \n",
    "            print(\"Input must not be less than 0\")\n",
    "            return\n",
    "        if int(x_unsafe) > self.max_val: \n",
    "            print(\"Input must not be greater than {}\".format(self.max_val))\n",
    "            return\n",
    "        x_safe = int(x_unsafe)\n",
    "        return x_safe\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            x0 = self._check_input(\"Please input x0 in range 0 to {}\".format(self.max_val))\n",
    "            x1 = self._check_input(\"Please input x1 in range 0 to {}\".format(self.max_val))\n",
    "            if x0 is not None and x1 is not None:\n",
    "                x0_bits = arr2inbits(np.array((x0,), dtype=self.np_dtype))\n",
    "                x1_bits = arr2inbits(np.array((x1,), dtype=self.np_dtype))\n",
    "                x = np.dstack([x0_bits, x1_bits])\n",
    "                print(\"x0 int val: {}\".format(x0))\n",
    "                print(\"x0 bin val: {}\".format(x0_bits))\n",
    "                print(\"x0 int val: {}\".format(x1))\n",
    "                print(\"x0 bin val: {}\".format(x1_bits))\n",
    "                predictions= sess.run(\n",
    "                [self.rnn.predictions_series],\n",
    "                feed_dict={\n",
    "                    self.rnn.x: x,\n",
    "                    self.rnn.initial_state: np.zeros([1, hidden_neurons])\n",
    "                })\n",
    "                rounded_predictions = np.around(predictions)\n",
    "                predictions_cleaned = np.reshape(np.array(rounded_predictions[0]), [32]).astype(self.np_dtype)\n",
    "                print(\"y  int val: {} and correct answer is {}\"\n",
    "                      .format(bits2arr(predictions_cleaned, self.dtype)[0], x0 + x1))\n",
    "                print(\"y  bin val: {}\".format(predictions_cleaned))\n",
    "                #DO CALCULATION\n",
    "            user_input = input(\"Do you wish to perform a new calculation? (y/N)\")\n",
    "            if user_input not in \"Yy\" :\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input x0 in range 0 to 42949672959326\n",
      "Please input x1 in range 0 to 429496729556815\n",
      "x0 int val: 9326\n",
      "x0 bin val: [0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "x0 int val: 56815\n",
      "x0 bin val: [1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y  int val: 66141 and correct answer is 66141\n",
      "y  bin val: [1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Do you wish to perform a new calculation? (y/N)n\n"
     ]
    }
   ],
   "source": [
    "my_test_calc = RNNCalc(dtype, my_rnn)\n",
    "my_test_calc.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
